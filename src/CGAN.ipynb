{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from data import load_traindata\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from augmentation import augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c3d3490>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subclasses = 100\n",
    "epochs = 10000\n",
    "seq_size = 250\n",
    "batch_size = 32\n",
    "num_aug = 5\n",
    "split_size=0.8\n",
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9514, 28)\n"
     ]
    }
   ],
   "source": [
    "X, _ = load_traindata(num_subclasses)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "X = X.reshape(int(num_subclasses), int(1000 / seq_size), seq_size, 12)\n",
    "X = X.reshape(int(num_subclasses * int(1000 / seq_size)), seq_size, 12)\n",
    "train_size = int(split_size*len(X))\n",
    "test_size = len(X) - train_size\n",
    "X,X_test = random_split(X,[train_size, test_size])\n",
    "X = augment(X)\n",
    "X_input = X[:, :, 0]  # First channel\n",
    "Y_target = X[:, :, 1]  # Second channel\n",
    "\n",
    "\n",
    "X_test_tensors = [X_test.dataset[idx] for idx in range(len(X_test))] \n",
    "X_test_tensor = torch.stack(X_test_tensors) \n",
    "X_t = X_test_tensor[:, :, 0]  \n",
    "Y_t = X_test_tensor[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=1, num_filters=32):  # Modified in_channels\n",
    "        super(Generator, self).__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, num_filters, 4, 2, 1),  # 500 → 250\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters, num_filters * 2, 4, 2, 1),  # 250 → 125\n",
    "            nn.BatchNorm1d(num_filters * 2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 2, num_filters * 4, 4, 2, 1),  # 125 → 63\n",
    "            nn.BatchNorm1d(num_filters * 4),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 4, num_filters * 8, 4, 2, 1),  # 63 → 32\n",
    "            nn.BatchNorm1d(num_filters * 8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 8, num_filters * 8, 4, 2, 1),  # 32 → 16\n",
    "            nn.BatchNorm1d(num_filters * 8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc6 = nn.Sequential(\n",
    "            nn.Conv1d(num_filters * 8, num_filters * 8, 4, 2, 1),  # 16 → 8\n",
    "            nn.BatchNorm1d(num_filters * 8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(num_filters * 8, num_filters * 8, 4, 2, 1),  # 8 → 16\n",
    "            nn.BatchNorm1d(num_filters * 8),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(num_filters * 8, num_filters * 8, 4, 2, 1),  # 16 → 32\n",
    "            nn.BatchNorm1d(num_filters * 8),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(num_filters * 8, num_filters * 4, 4, 2, 1),  # 32 → 63\n",
    "            nn.BatchNorm1d(num_filters * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(num_filters * 4, num_filters * 2, 4, 2, 1, output_padding=1),  # 63 → 125\n",
    "            nn.BatchNorm1d(num_filters * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(num_filters * 2, num_filters, 4, 2, 1),  # 125 → 250\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec6 = nn.ConvTranspose1d(num_filters, out_channels, 4, 2, 1)  # 250 → 500\n",
    "        self.final_activation = nn.LeakyReLU(0.2)  # Use Tanh for normalized output\n",
    "\n",
    "    def match_size(self, x, target):\n",
    "        diff = x.size(2) - target.size(2)\n",
    "        if diff > 0:\n",
    "            return x[:, :, :-diff]\n",
    "        elif diff < 0:\n",
    "            return nn.functional.pad(x, (0, -diff))\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 2, seq_len)\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "\n",
    "        d1 = self.match_size(self.dec1(e6), e5) + e5\n",
    "        d2 = self.match_size(self.dec2(d1), e4) + e4\n",
    "        d3 = self.match_size(self.dec3(d2), e3) + e3\n",
    "        d4 = self.match_size(self.dec4(d3), e2) + e2\n",
    "        d5 = self.match_size(self.dec5(d4), e1) + e1\n",
    "        d6 = self.dec6(d5)\n",
    "        return self.final_activation(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, seq_size, num_filters=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=num_filters, kernel_size=4, stride=2, padding=1)\n",
    "        self.leakyRelu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_filters*2)\n",
    "        self.conv3 = nn.Conv1d(num_filters*2, num_filters*4, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_filters*4)\n",
    "        self.conv4 = nn.Conv1d(num_filters*4, num_filters*8, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm1d(num_filters*8)\n",
    "        self.conv5 = nn.Conv1d(num_filters*8, num_filters*8, 4, 2, 1)\n",
    "        self.bn4 = nn.BatchNorm1d(num_filters*8)\n",
    "        self.conv6 = nn.Conv1d(num_filters*8, 1, 4, 2, 1)\n",
    "        self.op = nn.Linear(3, 1)  # Correct input size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.leakyRelu(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.op(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.398145 M parameters for Generator\n",
      "0.437605 M parameters for Discriminator\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(X_input, Y_target)\n",
    "test_dataset = TensorDataset(X_t, Y_t)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator(seq_size).to(device)\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0005)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "print(sum(p.numel() for p in generator.parameters())/1e6, 'M parameters for Generator')\n",
    "print(sum(p.numel() for p in discriminator.parameters())/1e6, 'M parameters for Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(d_losses, g_losses):\n",
    "    def smooth_curve(points, factor=0.9):\n",
    "        smoothed = []\n",
    "        for point in points:\n",
    "            if smoothed:\n",
    "                smoothed.append(smoothed[-1] * factor + point * (1 - factor))\n",
    "            else:\n",
    "                smoothed.append(point)\n",
    "        return smoothed\n",
    "\n",
    "    plt.plot(smooth_curve(d_losses), label='D Loss')\n",
    "    plt.plot(smooth_curve(g_losses), label='G Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "def plotWave(X, Y, c=0):\n",
    "    x_np = X.squeeze(0).detach().cpu().numpy()\n",
    "    y_np = Y.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(x_np, color='blue', label='X (Generated Signal)')\n",
    "    plt.plot(y_np, color='red', label='Y (Real Signal)')\n",
    "    plt.legend()\n",
    "    plt.title(f'Wave Comparison - Frame {c}')\n",
    "    plt.show()\n",
    "\n",
    "    # MSE\n",
    "    mse = np.mean((x_np - y_np) ** 2)\n",
    "    # FD\n",
    "    fd = max(directed_hausdorff(x_np.reshape(-1, 1), y_np.reshape(-1, 1))[0],\n",
    "             directed_hausdorff(y_np.reshape(-1, 1), x_np.reshape(-1, 1))[0])\n",
    "\n",
    "    print(f\"Frame {c}:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Fréchet Distance: {fd:.4f}\")\n",
    "\n",
    "    return c + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    return torch.mean(torch.abs(x[:, 1:] - x[:, :-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Discrimiator Loss: 1.3817598819732666, Generator Loss: 0.7448569536209106\n",
      "Epoch 2/10000, Discrimiator Loss: 0.9144365787506104, Generator Loss: 1.910752296447754\n",
      "Epoch 3/10000, Discrimiator Loss: 0.659257709980011, Generator Loss: 3.731301784515381\n",
      "Epoch 4/10000, Discrimiator Loss: 0.3871452212333679, Generator Loss: 4.2038726806640625\n",
      "Epoch 5/10000, Discrimiator Loss: 0.2291051745414734, Generator Loss: 4.9710307121276855\n",
      "Epoch 6/10000, Discrimiator Loss: 0.13461874425411224, Generator Loss: 5.471002578735352\n",
      "Epoch 7/10000, Discrimiator Loss: 0.08614402264356613, Generator Loss: 6.002068519592285\n",
      "Epoch 8/10000, Discrimiator Loss: 0.059051111340522766, Generator Loss: 6.324778079986572\n",
      "Epoch 9/10000, Discrimiator Loss: 0.03583770617842674, Generator Loss: 6.503056526184082\n",
      "Epoch 10/10000, Discrimiator Loss: 0.03300109878182411, Generator Loss: 5.915240287780762\n",
      "Epoch 11/10000, Discrimiator Loss: 0.02315041795372963, Generator Loss: 6.520235538482666\n",
      "Epoch 12/10000, Discrimiator Loss: 0.021612536162137985, Generator Loss: 8.851637840270996\n",
      "Epoch 13/10000, Discrimiator Loss: 0.017504317685961723, Generator Loss: 6.921439170837402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m criterion(discriminator(d_fake_input), real_labels) \u001b[38;5;66;03m#+  0.1*total_variation_loss(fake_2)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 37\u001b[0m     \u001b[43moptimizer_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m g_losses\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     39\u001b[0m d_losses\u001b[38;5;241m.\u001b[39mappend(d_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BiometricByPass/.venv/lib/python3.9/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    378\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 379\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    382\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "g_losses, d_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    for real_1, real_2 in dataloader:\n",
    "        real_1, real_2 = real_1.to(device).float(), real_2.to(device).float()\n",
    "        real_2 += 0.01 * torch.randn_like(real_2)  # Noise augmentation\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Generate noise and condition\n",
    "        noise = torch.randn_like(real_1)\n",
    "        gen_input = torch.stack([real_1, noise], dim=1)  # (batch, 2, seq_len)\n",
    "        fake_2 = generator(gen_input)\n",
    "\n",
    "        # Real and fake inputs for discriminator\n",
    "        d_real_input = torch.cat([real_2.unsqueeze(1), real_1.unsqueeze(1)], dim=1)\n",
    "        d_fake_input = torch.cat([fake_2.detach(), real_1.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Discriminator loss\n",
    "        real_labels = torch.ones(real_1.size(0), 1, device=device)\n",
    "        fake_labels = torch.zeros(real_1.size(0), 1, device=device)\n",
    "\n",
    "        d_real_loss = criterion(discriminator(d_real_input), real_labels)\n",
    "        d_fake_loss = criterion(discriminator(d_fake_input), fake_labels)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        gen_input = torch.stack([real_1, torch.randn_like(real_1)], dim=1)\n",
    "        fake_2 = generator(gen_input)\n",
    "        d_fake_input = torch.cat([fake_2, real_1.unsqueeze(1)], dim=1)\n",
    "        g_loss = criterion(discriminator(d_fake_input), real_labels) #+  0.1*total_variation_loss(fake_2)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "    g_losses.append(g_loss.item())\n",
    "    d_losses.append(d_loss.item())\n",
    "    if (epoch+1)%100==0:\n",
    "        plot_losses(d_losses, g_losses)\n",
    "        with torch.no_grad():\n",
    "            for real_1, real_2 in dataloader:\n",
    "                real_1, real_2 = real_1.to(torch.float32),real_2.to(torch.float32)\n",
    "                real_1, real_2 = real_1.to(device), real_2.to(device)\n",
    "                gen_input = torch.stack([real_1, torch.randn_like(real_1)], dim=1) \n",
    "                fake_2 = generator(gen_input)\n",
    "                counter = plotWave(fake_2[0],real_2[0], counter)\n",
    "                counter = plotWave(fake_2[12],real_2[12], counter) #random output\n",
    "                break\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Discrimiator Loss: {d_loss.item()}, Generator Loss: {g_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
